{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from abc import abstractmethod, ABCMeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunction:\n",
    "    \n",
    "    __metaclass__ = ABCMeta\n",
    "    \n",
    "    @abstractmethod\n",
    "    def func(self, thetas):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def grad(self, thetas):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def line_search(self, thetas):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def dim(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class QuadraticCost(CostFunction):\n",
    "    def __init__(self, A, B, C):\n",
    "        self._A = A\n",
    "        self._B = B\n",
    "        self._C = C\n",
    "    \n",
    "    def func(self, thetas):\n",
    "        return (0.5*thetas.T.dot(self._A).dot(thetas) + thetas.T.dot(self._B) + self._C).item()\n",
    "    \n",
    "    def grad(self, thetas):\n",
    "        return self._A.dot(thetas) + self._B\n",
    "    \n",
    "    def line_search(self, thetas):\n",
    "        grad = self.grad(thetas)\n",
    "        return (grad.T.dot(grad)/(grad.T.dot(self._A).dot(grad))).item()\n",
    "    \n",
    "    def dim(self):\n",
    "        return len(self._B)\n",
    "\n",
    "\n",
    "class LinRegCost(QuadraticCost):\n",
    "    def __init__(self, X, y):\n",
    "        (m, n) = X.shape\n",
    "        \n",
    "        A = X.T.dot(X) / m\n",
    "        B = - X.T.dot(y) / m\n",
    "        C = - y.T.dot(y) / m\n",
    "        \n",
    "        QuadraticCost.__init__(self, A, B, C)\n",
    "\n",
    "\n",
    "class GradientDescent:\n",
    "    def __init__(self, cost, alpha=None):\n",
    "        self.cost = cost\n",
    "        self._const_alpha = alpha\n",
    "    \n",
    "    def alpha(self, theta):\n",
    "        alpha_ = self._const_alpha\n",
    "        if alpha_ is None:\n",
    "            alpha_ = self.cost.line_search(theta)\n",
    "        return alpha_\n",
    "        \n",
    "    def run(self, theta0, max_iter=1000, tol=1e-5):\n",
    "        theta = theta0\n",
    "        self.steps = [theta0]\n",
    "\n",
    "        iter_ = 1 # variable to keep track of number of iteration\n",
    "        while iter_ <= max_iter:\n",
    "            # Gradient descent update\n",
    "            theta_new = theta - self.alpha(theta) * self.cost.grad(theta)\n",
    "            \n",
    "            if abs(theta_new - theta).max() < tol:\n",
    "                break\n",
    "            else:\n",
    "                theta = theta_new\n",
    "            \n",
    "            # Update number of iterations\n",
    "            iter_ = iter_ + 1\n",
    "            self.steps.append(theta)\n",
    "        \n",
    "        return theta\n",
    "    \n",
    "    def cost_at_steps(self):\n",
    "        return [self.cost.func(theta) for theta in self.steps]\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    \n",
    "    def normal_eq(self, X, y):\n",
    "        self.thetas = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "    def gradient_descent(self, X, y):\n",
    "        cost = LinRegCost(X, y)\n",
    "        gd = GradientDescent(cost)\n",
    "        self.thetas = gd.run(np.zeros((cost.dim(), 1)))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return X.dot(self.thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(ds.data, axis=0)\n",
    "std = np.std(ds.data, axis=0)\n",
    "X = (ds.data - mu) / std\n",
    "(m, n) = X.shape\n",
    "y = ds.target.reshape((m, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.92041113],\n",
       "       [ 1.08098058],\n",
       "       [ 0.14296712],\n",
       "       [ 0.68220346],\n",
       "       [-2.06009246],\n",
       "       [ 2.67064141],\n",
       "       [ 0.02112063],\n",
       "       [-3.10444805],\n",
       "       [ 2.65878654],\n",
       "       [-2.07589814],\n",
       "       [-2.06215593],\n",
       "       [ 0.85664044],\n",
       "       [-3.74867982]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.normal_eq(X, y)\n",
    "lr.thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.92038364],\n",
       "       [ 1.08093373],\n",
       "       [ 0.14283377],\n",
       "       [ 0.6822231 ],\n",
       "       [-2.06006488],\n",
       "       [ 2.67066438],\n",
       "       [ 0.02110285],\n",
       "       [-3.10446144],\n",
       "       [ 2.6584445 ],\n",
       "       [-2.07550172],\n",
       "       [-2.06214093],\n",
       "       [ 0.85663564],\n",
       "       [-3.74866302]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2 = LinearRegression()\n",
    "lr2.gradient_descent(X, y)\n",
    "lr2.thetas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
